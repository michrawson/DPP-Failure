\documentclass{article}
%\usepackage{showkeys}
\usepackage{amsmath, amsfonts}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{enumerate}
\usepackage{subfig}
\usepackage[margin=1in]{geometry}

\begin{document}
\begin{center}
\Large
DPP Failure\\
\normalsize
~\\
Michael Rawson\\
~\\
Advisor: Prof. Carlos Fernandez-Granda
\end{center}

\subsection*{Introduction}
\subsubsection*{Regression}
\qquad ~ Regression methods are a set of techniques used to infer information about systems from collected data. They are commonly used in engineering, science, and statistics. Linear regression creates a linear model to analyze the relationship between predictors and the sample values (\ref{eq:reg}). If $n=p$ then a hyperplane through the data is calculated, if $A$ is full rank. If $n>p$ then often the $L_2$ norm of the difference is minimized, $\min_{x\in \mathbf R^p} \|Ax-y\|_2$. If $n<p$ then, from Gaussian elimination, the solution has infinite solutions, if $A$ is full rank. 

\begin{equation} \label{eq:reg}
\begin{bmatrix}
    a_{11}       & a_{12} & \dots & a_{1p} \\
    a_{21}       & a_{22} & \dots & a_{2p} \\
    \vdots	    & \vdots  &          & \vdots  \\
    a_{n1}       & a_{n2} & \dots & a_{np}
\end{bmatrix} \begin{bmatrix}
    x_{1} \\
    x_{2} \\
    \vdots \\
    \vdots \\
    \vdots \\
    x_{p}
\end{bmatrix}=\begin{bmatrix}
    y_{1} \\
    y_{2} \\
    \vdots \\
    y_{n}
\end{bmatrix}
\end{equation}
where $a_{ij}$ is the value of predictor $i$ for sample $j$\\
and $x_i$ is the weight for predictor $i$\\
and $y_i$ is the $i^{th}$ sample value\\

\quad Regularization is often used in data analysis to simplify a complex model and to avoid over-fitting. Overfitting happens when a model learns not just the underlying signal, but the noise too. If there is no noise, then a linear interpolation could be directly applied, to a linear problem. Ridge Regression dampens $x$ by regularizing the $L_2$ norm of $x$ in the objective function (\ref{eq:ridge}). 

\begin{equation} \label{eq:ridge}
\min_{x\in R^p} \|Ax-y\|_2^2 + \lambda \|x\|_2^2
\end{equation}
where $\lambda$ is a hyper-parameter.\\

\quad Sometimes there is reason to believe that the relevant predictors to the sample value should be sparse. In order to find a sparse solution, often the $L_1$ norm of $x$ is also minimized in the objective function. The least absolute shrinkage and selection operator (LASSO) adds the weighted $L_1$ norm of $x$ (\ref{eq:lasso}). 

\begin{equation} \label{eq:lasso}
\min_{x\in R^p} \|Ax-y\|_2^2 + \lambda \|x\|_1
\end{equation}
where $\lambda$ is a hyper-parameter.\\

However, if predictors are identical, then the LASSO problem doesn't have a unique solution. There are infinite solutions. This does not achieve the objective of learning about the relationship between the predictors and data. Strongly correlated predictors similarly create non-unique solutions due to limited machine precision. One way to avoid this uniqueness problem is to use an objective function that penalizes both the $L_1$ norm and $L_2$ norm of $x$, know as the Elastic Net (\ref{eq:enet}).

\begin{equation} \label{eq:enet}
\min_{x\in R^p} \|Ax-y\|_2^2 + \lambda_1 \|x\|_1 + \lambda_2 \|x\|_2^2
\end{equation}
where $\lambda_1$ and $\lambda_2$ are hyper-parameters.\\

\subsubsection*{Screening}
\qquad ~ For large datasets, where the samples and/or predictors are large, computing time becomes a limitation. In order to be more computationally efficient many screening methods have been designed. Screening methods remove predictors ahead of optimizing the objective function, allowing practitioners to solve systems with many more predictors. Two types of screening methods are the safe methods and the heuristic methods. The safe methods guarantee to not discard an important predictor. The heuristic methods might discard important predictors, though post-processing can be employed to identify and correct failures. The strong rule \cite{strong} is an unsafe method that discards all predictors that satisfy (\ref{eq:strong}). 

\begin{equation} \label{eq:strong}
|a_j^T (y - Ax(\lambda'))| < 2\lambda - \lambda'
\end{equation}

The strong rule branches off the ``SAFE'' rule \cite{safe}:
$$ |a_j^T y| < \lambda - \|a_j\|_2 \|y\|_2 \frac{\lambda_{max}-\lambda}{\lambda_{max}} $$
And replaces $\|a_j\|_2 \|y\|_2/\lambda_{max}$ with $1$. Tibshirani et al. also propose sequential versions of ``SAFE'' and strong rules for efficiently solving the regression for many values of $\lambda$.

\subsubsection*{DPP}

The Dual Polytope Projection (DPP) \cite{ddp} is a theoretically safe method that discards all predictors that satisfy (\ref{eq:dpp}).\\
\\
Let $a_j$ is the $j^{th}$ column of $A$ \\
and $\lambda' = \gamma \lambda$ and $\gamma > 1$ \\ 
and $x(\lambda')$ be the solution for $\lambda'$.\\

\begin{equation} \label{eq:dpp}
|a_j^T (y - Ax(\lambda'))| < \lambda' - \|a_j\|_2 \|y\|_2 \frac{\lambda'-\lambda}{\lambda}
\end{equation}

This comes from the following.
Start with the dual problem:
\begin{equation} \label{eq:dual}
\theta^*(\lambda)=\texttt{argsup}_\theta \left\{\frac 12 \|y\|_2^2 - \frac {\lambda^2}{2}\left\|\theta-\frac{y}{\lambda}\right\|_2^2 : |a^T_i \theta|\le 1, i = 1,2,...,p \right\}
\end{equation}
KKT conditions give:
$$ y = A\beta^{*}(\lambda) + \lambda\theta^*(\lambda) $$
where $ \beta^{*}(\lambda) $ is the solution of Eq. \ref{eq:lasso}.\\
With some relaxation, $\sup_{\theta\in\Theta}|a_i^T\theta|<1$ implies an inactive feature/predictor $i$.\\
\\
The dual optimal solution can be expressed with a projection operator $P_F$ to the feasible set of Eq. (\ref{eq:dual}) by:
$$ \theta^*(\lambda) = P_F(y/\lambda) = \texttt{argmin}_{\theta\in F}\|\theta - \frac y\lambda\|_2 $$
From nonexpansiveness of projection operators defined in a Hilbert space with respect to a nonempty closed and convex set:
$$ \|\theta^*(\lambda)-\theta^*(\lambda')\|_2 = \|P_F(y/\lambda)-P_F(y/\lambda')\|_2 \le \|(y/\lambda)-(y/\lambda')\|_2 = \left| \frac{1}{\lambda}-\frac{1}{\lambda'} \right| \|y\|_2 $$
Then $\theta^*(\lambda)$ is in the following ball:
$$ \theta^*(\lambda)\in B\left(\theta^*(\lambda'),\left| \frac{1}{\lambda}-\frac{1}{\lambda'} \right| \|y\|_2\right) $$
From KKT relaxation above, we bound $|a_i^T\theta^*(\lambda)|$ with the ball from the information from $\lambda'$.
$$ |a_i^T\theta^*(\lambda)| < 1 - \|a_i\|_2\|y\|_2\left|\frac{1}{\lambda}-\frac{1}{\lambda'}\right| $$
Then use KKT for $\theta^*$ and we have DPP.\\
~\\

Furthermore, Enhanced DPP (EDPP) \cite{ddp} is an extension to DPP that discards all predictors that satisfy (\ref{eq:edpp}).\\

\begin{equation} \label{eq:edpp}
\left|a_j^T \left(\frac{y - Ax(\lambda')}{\lambda}\right)\right| < 1 - \frac 12 \|a_j\|_2 \|v_2^{\perp}\|_2
\end{equation}
where $v_1(\lambda') = \frac{y}{\lambda'} - \theta(\lambda')$ for $\lambda' \in (0, \lambda_{\max})$\\
and $v_2(\lambda,\lambda') = \frac{y}{\lambda} - \theta(\lambda')$\\
and $v_2^{\perp}(\lambda,\lambda') = v_2(\lambda,\lambda') - \frac{\left<v_1(\lambda'),v_2(\lambda,\lambda')\right>}{\|v_1(\lambda')\|^2_2} v_1(\lambda')$\\
and dual solution $\theta(\lambda') = \frac{y-Ax(\lambda')}{\lambda'} $ from KKT conditions \\

There are `global' versions of DPP rules that use $\lambda_{max}$ but we won't consider them since the rules quickly degrade as $\lambda$ parts with $\lambda_{max}$.



\subsection*{Failure Analysis}

\qquad ~ The DPP and EDPP sequential rules utilize previously computed solutions $x$ for larger $\lambda$. However, if only an approximate of the previous objective function's solution is known, then DPP and EDPP can fail.\\

Let $0<\lambda<\lambda'$ \\
Let the features/predictors, $a_j$, and response, $y$ be normalized. $\|a_j\|_2=1$ and $\|y\|_2=1$\\
Let $x$ be the solution for $\lambda'$ and assume we only have access to an approximation, $x+\epsilon$ for $\epsilon\in \mathbf R^p$ and $0<|\epsilon_i|<<1$. \\
Then for $\lambda'$ we have $\|Ax-y\|_2>0$ and let $A(x+\epsilon)-y = \delta \in \mathbf R^p$ and $0<|\delta_i|<<1$

\subsubsection*{DPP Failure}
We examine the failure of the DPP rules.\\
From
$$ |a_j^T (y - Ax(\lambda'))| < \lambda' - \|a_j\|_2 \|y\|_2 \frac{\lambda'-\lambda}{\lambda} $$
$$ |a_j^T (y - A(x+\epsilon))| < \lambda' - \|a_j\|_2 \|y\|_2 \frac{\lambda'-\lambda}{\lambda} $$
$$ |a_j^T \delta| < \lambda' - \frac{\lambda'-\lambda}{\lambda} $$
Take the limit
$$ \lim_{\delta \rightarrow 0} |a_j^T \delta| = 0$$
So
$$ 0 < \lambda' - \frac{\lambda'-\lambda}{\lambda} $$
$$ 0 < 1 - \frac{\lambda'-\lambda}{\lambda\lambda'} $$
In the limit, for certain $\lambda'$ and $\lambda$ the rule always evaluates to true, failing to safely screen out predictors. We plot the unsafe region in Fig. \ref{fig:DPP_failure_region_limit}. By continuity, there is a neighborhood of failure about $\delta$, $\lambda'$, and $\lambda$.

\begin{figure}
    \centering
    {\includegraphics[width=11cm]{DPP_failure_region_limit.eps} }
    \caption{DPP failure region in limit of $\delta$ for $\lambda'$ and $\lambda$ in yellow.}
	\label{fig:DPP_failure_region_limit}
\end{figure}


\subsubsection*{EDPP Failure}

We examine the failure of the EDPP rules.\\
Let
$$
v_1^{perturted}(\lambda') = \frac{y}{\lambda'} - \frac{y-A(x+\epsilon)}{\lambda'}
$$

$$
v_2^{perturted}(\lambda,\lambda') = \frac{y}{\lambda} - \frac{y-A(x+\epsilon)}{\lambda'}
$$

$$
v_2^{\perp,perturted}(\lambda,\lambda') = v_2^{perturted}(\lambda,\lambda') - \frac{\left<v_1^{perturted}(\lambda'),v_2^{perturted}(\lambda,\lambda')\right>}{\|v_1^{perturted}(\lambda')\|^2_2} v_1^{perturted}(\lambda')
$$
Rule:
$$
\left|a_j^T \left(\frac{y - A(x+\epsilon)}{\lambda}\right)\right| < 1 - \frac 12 \|a_j\|_2 \left\| v_2^{\perp,perturted}(\lambda,\lambda') \right\|_2
$$
\\
Let $\gamma = \frac {\lambda'}{\lambda}$. So $\gamma>1$.\\
Left Hand Side:
$$
\left|a_j^T \left(\frac{y - A(x+\epsilon)}{\lambda}\right)\right| = \left|a_j^T \left(\frac{\delta}{\lambda}\right)\right| 
$$
\\
Right Hand Side: \\
\\
$$
1 - \frac 12 \|a_j\|_2 \left\| v_2^{\perp,perturted}(\lambda,\gamma\lambda) \right\|_2 
= 1 - \frac 12 \left\| \frac y\lambda - \frac {\delta}{\gamma\lambda} - \frac {\left<\frac{y-\delta}{\gamma\lambda},\frac{y}{\lambda}-\frac{\delta}{\gamma\lambda}\right>}{\|\frac{y-\delta}{\gamma\lambda}\|_2^2}  \frac{y-\delta}{\gamma\lambda}  \right\|_2
$$
Take the limit
$$
\lim_{\delta\rightarrow 0} 1 - \frac 12 \left\| \frac y\lambda - \frac {\delta}{\gamma\lambda} - \frac {\left<\frac{y-\delta}{\gamma\lambda},\frac{y}{\lambda}-\frac{\delta}{\gamma\lambda}\right>}{\|\frac{y-\delta}{\gamma\lambda}\|_2^2}  \frac{y-\delta}{\gamma\lambda}  \right\|_2 
= 1 - \frac 12 \left\| \frac y\lambda - \frac {\left<y,y\right>}{\lambda\|y\|_2^2}  y  \right\|_2=1
$$
Since\\
\\
$0<1$\\
\\
Then, in the limit, the rule always evaluates true and does not safely discard predictors. By continuity, there is a neighborhood of failure about $\delta$, $\lambda'$, and $\lambda$.

\subsection*{Experiment}

\subsubsection*{Synthetic Data}

\qquad ~ We experiment with DPP and EDPP on synthetic data. We use a random Gaussian i.i.d. matrix, $A\in R^{100 \times 500}$, and a random Gaussian i.i.d. sparse signal vector, $x\in R^{500}$ and $\|\texttt{sign}(x)\|_1=100$. We compute the product, $y$, normalize predictors, $a_j$, and response, $y$, and then solve the inverse problem with LASSO to reconstruct the scaled, sparse signal $x$. We utilize a high precision optimizer set to continue as long as it can make progress. However, the result is still limited by double precision. We find and record where DPP and EDPP fails to safely discard predictors for a neighborhood of $\lambda$ and $\lambda'$. See Fig. \ref{fig:DPP_fail} for DPP and Fig. \ref{fig:EDPP_fail} for EDPP. We notice that the failure region for EDPP is much larger than DPP, which is not surprising since EDPP is a tighter bound than DPP. We also notice that the neighborhood of complete failure is not necessarily bordered by a neighborhood of complete success. This is not surprising since, for each predictor, the rules depends on the dot product with predictors, $a_j$, and since these vary, on the boundary, we expect some to cross while others do not. This explains the partial failure around the neighborhood of complete failure. 

\begin{figure}
    \centering
    \subfloat{{\includegraphics[width=7cm]{DPP_fail_large.eps} }}
    \qquad
    \subfloat{{\includegraphics[width=7cm]{DPP_fail_small.eps} }}
    \caption{DPP failure region on synthetic data.}
	\label{fig:DPP_fail}
\end{figure}

\begin{figure}
    \centering
    {\includegraphics[width=7cm]{EDPP_fail.eps} }
    \caption{EDPP failure region on synthetic data.}
	\label{fig:EDPP_fail}
\end{figure}


\subsubsection*{Gas sensor array under flow modulation Data Set}

\qquad ~ We experiment with DPP and EDPP on a real dataset from a gas sensor array \cite{bioinspired}. We run a Lasso regression on each predictor against the remaining predictors. We choose the predictor with the lowest Lasso error. Then we test DPP and EDPP screening for failure regions. We plot DPP failure in Fig. \ref{fig:DPP_fail_real} and EDPP failure in Fig. \ref{fig:EDPP_fail_real}. The results are strikingly similar to the experiments with synthetic data.

\begin{figure}
    \centering
    \subfloat{{\includegraphics[width=7cm]{DPP_fail_large_real.eps} }}
    \qquad
    \subfloat{{\includegraphics[width=7cm]{DPP_fail_small_real.eps} }}
    \caption{DPP failure region on the `Gas sensor array under flow modulation Data Set' \cite{bioinspired} after normalizing the features/predictors and response.}
	\label{fig:DPP_fail_real}
\end{figure}

\begin{figure}
    \centering
    {\includegraphics[width=7cm]{EDPP_fail_real.eps} }
    \caption{EDPP failure region on the `Gas sensor array under flow modulation Data Set' \cite{bioinspired} after normalizing the features/predictors and response.}
	\label{fig:EDPP_fail_real}
\end{figure}

\subsection*{Conclusion}
We conclude that sequential DPP and EDPP rules are potentially useful screening rules but cannot be trusted to safely screen out predictors due to finite precision and limited optimizers. The KKT conditions can be checked on each predictor after an unsafe screening to identify mistakes \cite{strong}. If a mistake is found then adding the predictor back and rerunning the regression is required solve the inverse problem. Future directions might include finding a safe region with respect to $\lambda'$ and $\lambda$ for DPP and EDPP. Another future direction might be checking for failure in other safe screening rules like \cite{safe}.

\bibliographystyle{plain}
\bibliography{Master}

\end{document}

